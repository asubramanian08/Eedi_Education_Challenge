{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Constants\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "!wget https://dqanonymousdata.blob.core.windows.net/neurips-public/data.zip\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break cell execution: Credit https://stackoverflow.com/questions/24005221/ipython-notebook-early-exit-from-cell/56953105#56953105\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "TRAIN = pd.read_csv('data/train_data/train_task_1_2.csv')\n",
    "TRAIN = TRAIN.rename({'UserId': 'StudentId'}, axis=1)\n",
    "assert len(TRAIN['AnswerId'].unique())==TRAIN.shape[0]\n",
    "# assert len(TRAIN['StudentId'].unique())==TRAIN.shape[0] # fails\n",
    "assert all(TRAIN['IsCorrect'].isin([0,1]))\n",
    "assert all(TRAIN['IsCorrect'] == (TRAIN['AnswerValue']==TRAIN['CorrectAnswer']))\n",
    "display(TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject Data\n",
    "SUBJECT = pd.read_csv('data/metadata/subject_metadata.csv').query('Level < 3')\n",
    "assert len(SUBJECT['SubjectId'].unique())==SUBJECT.shape[0]\n",
    "assert len(SUBJECT['Name'].unique())==SUBJECT.shape[0]\n",
    "assert SUBJECT['ParentId'].apply(lambda f: math.isnan(f) or f.is_integer).all()\n",
    "display(SUBJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Data\n",
    "QUESTION = pd.read_csv('data/metadata/question_metadata_task_1_2.csv')\n",
    "def limit_subjects(subject_list): # Use only level<3 subjects\n",
    "  return [x for x in subject_list if x in SUBJECT['SubjectId'].values]\n",
    "QUESTION['SubjectId'] = [limit_subjects(list(map(int,x[1:-1].split(', '))))\n",
    "                         for x in QUESTION['SubjectId']]\n",
    "assert len(QUESTION['QuestionId'].unique())==QUESTION.shape[0]\n",
    "assert all(QUESTION['SubjectId'].apply(type) == list)\n",
    "assert all(QUESTION['SubjectId'].apply(bool)) # SubjectId not empty\n",
    "assert len(TRAIN['QuestionId'].unique())==QUESTION.shape[0]\n",
    "QUESTION = QUESTION.set_index('QuestionId').sort_index()\n",
    "display(QUESTION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the subjects into each question\n",
    "subject_map = {x: y for x, y in zip(SUBJECT.SubjectId, SUBJECT.Name)}\n",
    "QUESTION = QUESTION.join(QUESTION.SubjectId.explode()\n",
    "  .apply(lambda x: subject_map[x]).str.get_dummies()\n",
    "  .groupby(level=0).sum().astype(bool))\n",
    "temp = QUESTION.iloc[0]['SubjectId'] + [1189, 130]\n",
    "results = [QUESTION[subject_map[x]].iloc[0] for x in temp]\n",
    "assert results == [True, True, True, False, False]\n",
    "QUESTION = QUESTION.drop(columns=['SubjectId'])\n",
    "assert QUESTION.shape[1] == SUBJECT.shape[0]\n",
    "assert max(QUESTION.index) == QUESTION.shape[0] - 1\n",
    "assert QUESTION.any().any()\n",
    "display(QUESTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create STUDENT features (average of correct answers per subject)\n",
    "STUDENT = TRAIN.groupby('StudentId')['IsCorrect'].agg(average='mean', num_questions='count')\n",
    "assert len(TRAIN['StudentId'].unique())==STUDENT.shape[0]\n",
    "assert max(STUDENT.index) == STUDENT.shape[0] - 1\n",
    "for _, x in sorted(subject_map.items(), key = lambda x: x[1]):\n",
    "    select = set(QUESTION.loc[QUESTION[x] == True].index.values)\n",
    "    averages = TRAIN[TRAIN['QuestionId'].isin(select)].groupby('StudentId')['IsCorrect'].mean()\n",
    "    STUDENT = pd.concat((STUDENT, averages), axis=1)\n",
    "    STUDENT.rename(columns={'IsCorrect': x}, inplace=True)\n",
    "assert STUDENT.shape[1] - 2 == SUBJECT.shape[0]\n",
    "display(STUDENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High use values\n",
    "FEATURE_COUNT = len(subject_map)\n",
    "QUESTION_COUNT = QUESTION.shape[0]\n",
    "STUDENT_COUNT = STUDENT.shape[0]\n",
    "average = TRAIN['IsCorrect'].mean()\n",
    "print(FEATURE_COUNT, QUESTION_COUNT, STUDENT_COUNT, average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DATA variable\n",
    "DATA = SUBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information\n",
    "print('What is the type of the data: ' + str(type(DATA)))\n",
    "print('Structure of the data: ' + str(DATA.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General information\n",
    "DATA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 entries\n",
    "DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue exploring the data here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Understanding Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Solution: Predictions are the average answer for a question\n",
    "test_nn = False\n",
    "print(\"RUN \\\"TEST THE MODEL\\\" CELLS TO TEST THE NAIVE SOLUTION\")\n",
    "NAIVE_PRED = TRAIN.groupby('QuestionId')['IsCorrect'].mean()\n",
    "print(NAIVE_PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of correct answers\n",
    "QUESTION_INFO = TRAIN.groupby('QuestionId')['IsCorrect'].agg(['mean', 'count', 'sum'])\n",
    "QUESTION_INFO = QUESTION_INFO.sort_values(by='mean')\n",
    "plt.plot(QUESTION_INFO['mean'].values)\n",
    "plt.title('Mean of correct answers per question')\n",
    "plt.xlabel('User Number')\n",
    "plt.ylabel('Average Correct Answers')\n",
    "plt.grid(color='gray', linestyle='-', linewidth=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject Integrity: Quantity vs. Diversity\n",
    "num_responces = TRAIN.groupby('QuestionId')['QuestionId'].count().values\n",
    "quantity = QUESTION.mul(num_responces, axis=0).sum()\n",
    "diversity = QUESTION.sum(axis=0).values\n",
    "plt.scatter(diversity, quantity)\n",
    "plt.title('Understanding the Subjects')\n",
    "plt.xlabel('Number of questions about the subject')\n",
    "plt.ylabel('Total student responces to the subject')\n",
    "plt.grid(color='gray', linestyle='-', linewidth=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model: Credit Coursera Machine Learning Specialization\n",
    "num_outputs = 32\n",
    "student_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=num_outputs, activation='sigmoid'),\n",
    "])\n",
    "question_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=num_outputs, activation='sigmoid'),\n",
    "])\n",
    "# create the user input and point to the base network\n",
    "input_student = tf.keras.layers.Input(shape=(FEATURE_COUNT))\n",
    "vs = tf.linalg.l2_normalize(student_NN(input_student), axis=1)\n",
    "# create the item input and point to the base network\n",
    "input_question = tf.keras.layers.Input(shape=(FEATURE_COUNT))\n",
    "vq = tf.linalg.l2_normalize(question_NN(input_question), axis=1)\n",
    "# compute the dot product of the two vectors vs and vq\n",
    "output = tf.keras.layers.Dot(axes=1)([vs, vq])\n",
    "# specify the inputs and output of the model and compile\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "model = keras.Model([input_student, input_question], output)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='BinaryCrossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "test_nn = True\n",
    "for epoch in range(10):\n",
    "    print(\"EPOCH\", epoch + 1)\n",
    "    for chunk in np.array_split(TRAIN, 10):\n",
    "        X_student = chunk.join(STUDENT, on='StudentId', how='inner', sort=True).iloc[:, 8:].fillna(average)\n",
    "        X_question = chunk.join(QUESTION, on='QuestionId', how='inner', sort=True).iloc[:, 6:].fillna(average)\n",
    "        assert X_student.shape[1] == X_question.shape[1] == SUBJECT.shape[0]\n",
    "        Y = chunk['IsCorrect']\n",
    "        assert X_student.shape[0] == X_question.shape[0] == len(Y)\n",
    "        model.fit([X_student, X_question], Y)\n",
    "        print('Chunk average:', chunk['IsCorrect'].mean())\n",
    "        # raise StopExecution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "TEST = pd.read_csv('data/test_data/test_public_answers_task_1.csv')\n",
    "TEST = TEST.rename({'UserId': 'StudentId'}, axis=1)\n",
    "assert len(TEST['QuestionId'].unique()) <= QUESTION.shape[0]\n",
    "assert TEST['StudentId'].isin(STUDENT.index).all()\n",
    "display(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "pred_correct, pred_count = 0, 0\n",
    "conf_mtx = [[0, 0], [0, 0]] # confusion matrix\n",
    "for chunk in np.array_split(TEST, 5):\n",
    "    if test_nn: # Test Neural Network\n",
    "        X_student = chunk.join(STUDENT, on='StudentId', how='inner', sort=True).iloc[:, 6:].fillna(average)\n",
    "        X_question = chunk.join(QUESTION, on='QuestionId', how='inner', sort=True).iloc[:, 4:].fillna(average)\n",
    "        assert X_student.shape[1] == X_question.shape[1] == SUBJECT.shape[0]\n",
    "        chunk['Submission'] = model.predict([X_student, X_question]).round()\n",
    "        assert all(chunk['Submission'].isin([0,1]))\n",
    "    else: # Test Naive Model\n",
    "        chunk['Submission'] = chunk['QuestionId'].map(NAIVE_PRED).round()\n",
    "    conf_mtx[0][0] += np.sum((chunk['IsCorrect']==0) & (chunk['Submission']==0))\n",
    "    conf_mtx[0][1] += np.sum((chunk['IsCorrect']==0) & (chunk['Submission']==1))\n",
    "    conf_mtx[1][0] += np.sum((chunk['IsCorrect']==1) & (chunk['Submission']==0))\n",
    "    conf_mtx[1][1] += np.sum((chunk['IsCorrect']==1) & (chunk['Submission']==1))\n",
    "    pred_correct += np.sum(chunk['IsCorrect']==chunk['Submission'])\n",
    "    pred_count += chunk.shape[0]\n",
    "    # if test_nn:\n",
    "    #     raise StopExecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results: Credit Eedi Starter Kit\n",
    "accuracy = pred_correct / pred_count\n",
    "print(\"accuracy:\", accuracy)\n",
    "print(\"correct:\", pred_correct, \"  \", \"total:\", pred_count)\n",
    "conf_mtx = np.divide(conf_mtx, pred_count)\n",
    "display(pd.DataFrame(conf_mtx, index=['true_0', 'true_1'], columns=['pred_0', 'pred_1']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1bb661b5f7809a14344d07c21fe33ba351569652c37de5ed4a9af6881070c59a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
